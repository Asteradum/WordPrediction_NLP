A <- matrix(2,1,1,2)
A
?"matrix"
A <- matrix(data = c(2,1,1,2), 2,2)
A
is.positive.definite(A)
install.packages("matrixcalc")
library(matrixcalc)
A <- matrix(data = c(2,1,1,2), 2,2)
is.positive.definite(A)
?transpose
At <- t(A)
At
svd(A)
Cholesky(A)
Cholesky(A)
chol(A)
B * t(B)
B <- chol(A)
B * t(B)
B <- chol(A, pivot = FALSE, ...)
B <- chol(A, pivot = FALSE)
B * t(B)
B <- chol(A, pivot = T)
B * t(B)
B %*%  t(B)
B <- chol(A, pivot = F)
View(B)
t(B)
B %*%  B
B
t(B)
B %*%  t(B)
B *  t(B)
B%*%A
C<-matrix(data = c(2,0,0,6,1,0,-8,5,3), 3,3)
View(C)
C<-matrix(data = c(2,6,-8,0,1,5,0,0,3), 3,3)
View(C)
t(C)
C %*% t(C)
D <- C %*% t(C)
chol(D)
is.positive.semi.definite(A)
Conj(t(B))
B %*%  Conj(t(B))
B %*%  t(B)
Conj(t(A))
Act <- Conj(t(A))
A=Act
A==Act
identical(A, Act)
identical(A, At) #Hermitian
?is.positive.definite
func(is.positive.definite)
getMethod(is.positive.definite)
getMethod("is.positive.definite")
showMethods("is.positive.definite")
is.positive.definite
svd(A)
z <- svd(A)
z[1]
z[2] %*% t(z[2])
class(z[2])
class(z[[2])
class(z[[2]])
z[[2]] %*% t(z[[2]])
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
?rnorm
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
bins <- seq(min(x), max(x), length.out = 11 + 1)
x    <- faithful[, 2]  # Old Faithful Geyser data
bins <- seq(min(x), max(x), length.out = 11 + 1)
rnorm(10,  breaks = bins, col = 'darkgray', border = 'white')
?break
?breaks
??breaks
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
?if
;
?if
;
??if
;
?if()
?lnorm
??lnorm
rexp(10)
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
bins <- seq(min(dist), max(dist), length.out = input$bins + 1)
hist(dist, breaks = bins, col = col, border = 'white')
col <- "steelblue2"
dist <-  rnorm(input$obs)
dist <-  rnorm(100)
hist(dist, breaks = bins, col = col, border = 'white')
bins <- seq(min(dist), max(dist), length.out = input$bins + 1)
bins <- seq(min(dist), max(dist), length.out = 10 + 1)
hist(dist, breaks = bins, col = col, border = 'white')
hist$col
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
shiny::runApp('Coursera/Developing_Data_Products/ShinyApp')
title       : Distribution Histogram Generator
colsums(a)
colSums(a)
install.packages("C:/Users/User/Downloads/DDPQuiz3_1.0.zip", repos = NULL, type = "win.binary")
createmean
library(DDPQuiz3)
install.packages(c("car", "caret", "CORElearn", "curl", "doParallel", "foreach", "forecast", "iterators", "lme4", "MatrixModels", "quantreg", "randomForest", "Rcpp", "RcppArmadillo", "RcppEigen", "rmarkdown", "scales"))
install.packages("C:/Users/User/Downloads/DDPQuiz3_1.0.zip", repos = NULL, type = "win.binary")
library(DDPQuiz3)
lm
colSums
dgamma
?getMethod()
?showMethods()
?lm
?dgamma
UseMethod(lm)
UseMethod(dgamma)
UseMethod("dgamma")
UseMethod("lm")
?UseMethod
?mean()
?lm()
?dgamma
?colSums
lm
source('~/.active-rstudio-document')
apply(blogs,2,max)
apply(blogs,MARGIN=C(2),max)
apply(blogs,2,length)
apply(blogs,2,nchar)
max(apply(blogs,2,nchar))
max(apply(news,2,nchar))
max(apply(twitter,2,nchar))
options( java.parameters = "-Xmx10000m" )
# Library Loading
library(tm)
library(RWeka)
library(wordcloud)
blogs <- readLines("../Source/final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul=TRUE)
blogs <- iconv(blogs, "latin1", "ASCII", sub="")
news <- readLines("../Source/final/en_US/en_US.news.txt", encoding = "UTF-8", skipNul=TRUE)
news <- iconv(news, "latin1", "ASCII", sub="")
twitter <- readLines("../Source/final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul=TRUE)
twitter <- iconv(twitter, "latin1", "ASCII", sub="")
blogs <- readLines("../Source/final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul=TRUE)
blogs <- iconv(blogs, "latin1", "ASCII", sub="")
news <- readLines("../Source/final/en_US/en_US.news.txt", encoding = "UTF-8", skipNul=TRUE)
news <- iconv(news, "latin1", "ASCII", sub="")
twitter <- readLines("../Source/final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul=TRUE)
twitter <- iconv(twitter, "latin1", "ASCII", sub="")
setwd("~/Coursera/WordPrediction_NLP/code")
blogs <- readLines("../Source/final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul=TRUE)
blogs <- iconv(blogs, "latin1", "ASCII", sub="")
news <- readLines("../Source/final/en_US/en_US.news.txt", encoding = "UTF-8", skipNul=TRUE)
news <- iconv(news, "latin1", "ASCII", sub="")
twitter <- readLines("../Source/final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul=TRUE)
twitter <- iconv(twitter, "latin1", "ASCII", sub="")
removeURLs <- function(x) gsub("http[s]?\\:\\/\\/[[:alnum:]]*", "", x)
removeEmails  <- function(x) gsub("[[:alnum:]]*@[[:alnum:]]+\\.[[:alnum:]]+", "", x)
removeUsersHashtags <- function(x) gsub("[@|\\#][[:alnum:]]+", "", x)
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
cleanCorpus <- function(corpus) {
# Load Profanity
fileURL <- "https://gist.githubusercontent.com/ryanlewis/a37739d710ccdb4b406d/raw/0fbd315eb2900bb736609ea894b9bde8217b991a/google_twunter_lol"
if (!file.exists("../Source/ProfanityList.txt"))
download.file(fileURL, destfile = "../Source/ProfanityList.txt")
profanity <- readLines("../Source/ProfanityList.txt", encoding = "UTF-8", skipNul=TRUE)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(removeURLs))
corpus <- tm_map(corpus, content_transformer(removeEmails))
corpus <- tm_map(corpus, content_transformer(removeUsersHashtags))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, profanity)
corpus <- tm_map(corpus, removePunctuation, preserve_intra_word_dashes = T)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(trim))
corpus
}
blogsSample <- blogs[1:100000]
newsSample <- news[1:100000]
tweetsSample <- twitter[1:100000]
textSample <- c(blogsSample, newsSample, tweetsSample)
rm(blogs)
rm(blogsFile)
rm(blogsWords)
rm(blogsSample)
rm(twitter)
rm(twitterFile)
rm(twitterWords)
rm(tweetsSample)
rm(news)
rm(newsFile)
rm(newsWords)
rm(newsSample)
corpus <- VCorpus(VectorSource(textSample))
corpus <- cleanCorpus(corpus)
Tokenizer <- function(textDF, tokens) {
#Create NGrams
NGrams <- NGramTokenizer(textDF,
Weka_control(min = tokens, max = tokens,
delimiters = " \\\r\n\t,:.?!\""))
#convert to table to group same hits
NGramsDistribution <- data.frame(table(NGrams))
#sort
NGramsDistribution <- NGramsDistribution[order(NGramsDistribution$Freq,
decreasing = TRUE),]
colnames(NGramsDistribution) <- c("NGram","Count")
NGramsDistribution
}
library( "RWeka" )
unigram <- Tokenizer(cleanSample, 1)
cleanSample <-data.frame(text=unlist(sapply(corpus,`[`, "content")),
stringsAsFactors = FALSE)
Tokenizer <- function(textDF, tokens) {
#Create NGrams
NGrams <- NGramTokenizer(textDF,
Weka_control(min = tokens, max = tokens,
delimiters = " \\\r\n\t,:.?!\""))
#convert to table to group same hits
NGramsDistribution <- data.frame(table(NGrams))
#sort
NGramsDistribution <- NGramsDistribution[order(NGramsDistribution$Freq,
decreasing = TRUE),]
colnames(NGramsDistribution) <- c("NGram","Count")
NGramsDistribution
}
library( "RWeka" )
unigram <- Tokenizer(cleanSample, 1)
bigram <- Tokenizer(cleanSample, 2)
trigram <- Tokenizer(cleanSample, 3)
quadgram <- Tokenizer(cleanSample, 4)
saveRDS(unigram, file = "../Staging/unigramBig.Rda")
saveRDS(bigram, file = "../Staging/bigramBig.Rda")
saveRDS(trigram, file = "../Staging/trigramBig.Rda")
saveRDS(quadgram, file = "../Staging/quadgramBig.Rda")
bigramRed <- bigram[bigram$Count>2,]
trigramRed <- trigram[trigram$Count>3,]
quadgramRed <- quadgram[quadgram$Count>4,]
saveRDS(bigramRed, file = "../Staging/ReducedBigramBig.Rda")
saveRDS(trigramRed, file = "../Staging/ReducedTrigramBig.Rda")
saveRDS(quadgramRed, file = "../Staging/ReducedQuadgramBig.Rda")
