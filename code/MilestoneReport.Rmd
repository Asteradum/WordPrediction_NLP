---
title: "Data Science Specialization Project - Milestone Report"
output: html_document
---

# Introduction

The following document contains the proccess followed to obtain, clean and understand the data provided for the Data Science Specialization - Capstone Project.

For this report it is important to understand the following points:

 - The data is contained in a ZIP file with multiple languages. As we are only interested in the English prediction, the rest of the languaes will be disregarded.
 - The objective is to been able to predict the next word of a sentence. In order to do so, we will need to process our data, work with it and create a model. 
 - The model will be based in NGrams. An Ngram are set of N words and depending how often they are repeated, it is more likely that a user would like to make use of it.

# Process 
 
## Data Acquisition and Environment Preparation 

The file weights more than 500 MB therefore, in case this file has been downloaded previously, this step will be skipped.

```{r, warning=FALSE,message=FALSE}
library(tm)
library(RWeka)
library(wordcloud)

fileURL <- "http://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
if (!file.exists("../Source/Coursera-SwiftKey.zip")) 
{
  download.file(fileURL, destfile = "../Source/Coursera-SwiftKey.zip")
  unzip("../Source/Coursera-SwiftKey.zip")
}
rm(fileURL)
```

## Basic Summary

Once the data is downloaded, we need to start analyzing it. The English source are contained in three different text files, depending on the source. This basic summary provides an estimation of size, words count and lines for each of the files.

```{r, warning=FALSE, cache=TRUE}
# Filesize (Divided to get MB)
blogsFile <- file.info("../Source/final/en_US/en_US.blogs.txt")$size / 1024^2
newsFile <- file.info("../Source/final/en_US/en_US.news.txt")$size / 1024^2
twitterFile <- file.info("../Source/final/en_US/en_US.twitter.txt")$size / 1024^2

# Words (extra line for emoticons)
blogs <- readLines("../Source/final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul=TRUE)
blogs <- iconv(blogs, "latin1", "ASCII", sub="")
news <- readLines("../Source/final/en_US/en_US.news.txt", encoding = "UTF-8", skipNul=TRUE)
news <- iconv(news, "latin1", "ASCII", sub="")
twitter <- readLines("../Source/final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul=TRUE)
twitter <- iconv(twitter, "latin1", "ASCII", sub="")

blogsWords <- sum(sapply(gregexpr("[[:alpha:]]+", blogs), function(x) sum(x > 0)))
newsWords <- sum(sapply(gregexpr("[[:alpha:]]+", news), function(x) sum(x > 0)))
twitterWords <- sum(sapply(gregexpr("[[:alpha:]]+", twitter), function(x) sum(x > 0)))

# Summary
summary <- data.frame(
  file = c("Blogs","News","Twitter"),
  size_MB = c(blogsFile, newsFile, twitterFile),
  words = c(blogsWords, newsWords, twitterWords),
  lines = c(length(blogs), length(news), length(twitter))
)

summary
```

## Sample Creation and Cleaning

As we can observe, the source are quite big, so before continue with the analysis, let's pick a manageable sample for each of the files. It is assumed that the data collected in the sources is random, so the first 10000 will be picked. Also, some variable cleaning is done in this step.

```{r}
#Take a Sample. Consider the distribution of the text random. Picking the first 10K of each
blogsSample <- blogs[1:10000]
newsSample <- news[1:10000]
tweetsSample <- twitter[1:10000]
textSample <- c(blogsSample, newsSample, tweetsSample)

#releasing space
rm(blogs)
rm(blogsFile)
rm(blogsWords)
rm(blogsSample)
rm(twitter)
rm(twitterFile)
rm(twitterWords)
rm(tweetsSample)
rm(news)
rm(newsFile)
rm(newsWords)
rm(newsSample)
```

As the text contains some undesired characters (commas, punctuation marks, emails, twitter users, hashtags...) it needs to be cleaned. Also, for this project it has been requested to remove profanity words. a list created by oogle has been used.

```{r, cache=TRUE, warning=FALSE}
removeURLs <- function(x) gsub("http[s]?\\:\\/\\/[[:alnum:]]*", "", x)
removeEmails  <- function(x) gsub("[[:alnum:]]*@[[:alnum:]]+\\.[[:alnum:]]+", "", x) 
removeUsersHashtags <- function(x) gsub("[@|\\#][[:alnum:]]+", "", x) 
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
cleanCorpus <- function(corpus) {
  # Load Profanity
  fileURL <- "https://gist.githubusercontent.com/ryanlewis/a37739d710ccdb4b406d/raw/0fbd315eb2900bb736609ea894b9bde8217b991a/google_twunter_lol"
  if (!file.exists("../Source/ProfanityList.txt")) 
    download.file(fileURL, destfile = "../Source/ProfanityList.txt")
  profanity <- readLines("../Source/ProfanityList.txt", encoding = "UTF-8", skipNul=TRUE)
  
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, content_transformer(removeURLs))
  corpus <- tm_map(corpus, content_transformer(removeEmails))
  corpus <- tm_map(corpus, content_transformer(removeUsersHashtags))
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, profanity)
  corpus <- tm_map(corpus, removePunctuation, preserve_intra_word_dashes = T)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, content_transformer(trim))
  corpus
}
corpus <- VCorpus(VectorSource(textSample))
corpus <- cleanCorpus(corpus)
```

## NGram Creation

The data has been cleaned, so it is time to start creating the NGrams. We are going to create unigrams, bigrams, trigram and quadgrams. Also, the structures will be sorted by frequency of the Ngram used in the sample

```{r, eval=FALSE}
#Convert into dataframe for working with RWeka 
cleanSample <-data.frame(text=unlist(sapply(corpus,`[`, "content")), 
                         stringsAsFactors = FALSE)

Tokenizer <- function(textDF, tokens) {
  #Create NGrams
  NGrams <- NGramTokenizer(textDF, 
                                  Weka_control(min = tokens, max = tokens, 
                                               delimiters = " \\\r\n\t,:.?!\""))
  #convert to table to group same hits
  NGramsDistribution <- data.frame(table(NGrams))
  #sort
  NGramsDistribution <- NGramsDistribution[order(NGramsDistribution$Freq, 
                                       decreasing = TRUE),]
  colnames(NGramsDistribution) <- c("NGram","Count")
  NGramsDistribution
}

unigram <- Tokenizer(cleanSample, 1)
bigram <- Tokenizer(cleanSample, 2)
trigram <- Tokenizer(cleanSample, 3)
quadgram <- Tokenizer(cleanSample, 4)

saveRDS(unigram, file = "../Staging/unigram.Rda")
saveRDS(bigram, file = "../Staging/unigram.Rda")
saveRDS(trigram, file = "../Staging/unigram.Rda")
saveRDS(quadgram, file = "../Staging/unigram.Rda")
```

## Analysis on Unigram

Once we have the Ngrams created, let's analyze the frequency of the words. we are going to take the paretto approximation. usually the 20% of the data contains the 80% requried information. 

We are going to calculate how many words are necessary to hold the 80% of the cummulative frequency.

```{r}
unigram <- readRDS("../Staging/unigram.Rda")
totalwords <- sum(unigram$Count)
paretto80 <- totalwords*.8
i <-1
cummulative <- 0
while (cummulative < paretto80)
{
  cummulative <- cummulative +  unigram[i,2]
  i<-i+1
}
paretto20 <- i / nrow(unigram)

paretto <- cbind(paretto20,i, nrow(unigram), paretto80, totalwords)
colnames(paretto)<- c("P20", "P20 Value", "Unique Words(P100 Value)",  "P80 Value", "Total Words (P100 Value)" )
paretto
```

We can observe that with a very small set of words, we can hold over the 80% of the cummulative frequency.

## Plots

We can visualize the created NGrams in wordsclouds, having a easy approach to the most used set of words. The wordclouds are limited by frequency and number of words.

```{r}
unigram <- readRDS("../Staging/unigram.Rda")
wordcloud(unigram$NGram,unigram$Count,
          min.freq=5,
          max.words=100,
          random.order=FALSE,
          colors=brewer.pal(12, "Paired"))
```

```{r}
bigram <- readRDS("../Staging/bigram.Rda")
wordcloud(bigram$NGram,bigram$Count,
          min.freq=5,
          max.words=100,
          random.order=FALSE,
          colors=brewer.pal(9, "Set3"))
```

```{r}
trigram <- readRDS("../Staging/trigram.Rda")
wordcloud(trigram$NGram,trigram$Count,
          scale= c(3,.25),
          min.freq=5,
          max.words=50,
          random.order=FALSE,
          colors=brewer.pal(8, "Dark2"))
```

```{r}
quadgram <- readRDS("../Staging/quadgram.Rda")
wordcloud(quadgram$NGram,quadgram$Count,
          scale= c(2,.2),
          min.freq=5,
          max.words=50,
          random.order=FALSE,
          colors=brewer.pal(8, "Paired"))
```
